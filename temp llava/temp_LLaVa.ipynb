{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d3f1986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "IMAGE_FOLDER = Path(r\"C:\\Users\\vivek\\OneDrive\\Desktop\\Multimodal Conversational AI\\temp llava\\images\")\n",
    "output_dir = Path(r\"C:\\Users\\vivek\\OneDrive\\Desktop\\Multimodal Conversational AI\\temp llava\\normalized_images\")\n",
    "image_files = [\"chest_xray_001.jpg\", \"chest_xray_002.jpg\", \"chest_xray_003.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce5b78e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008734b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and variables\n",
    "MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"\n",
    "#DATASET_PATH = \"./dataset.json\"  # Path to your JSON dataset\n",
    "DATASET_PATH = Path(r\"C:\\Users\\vivek\\OneDrive\\Desktop\\Multimodal Conversational AI\\temp llava\\openi-instruct.json\")\n",
    "#IMAGE_FOLDER = \"./dataset/images\"  # Folder containing images\n",
    "OUTPUT_DIR = \"llava-7b-finetuned\"\n",
    "MAX_LENGTH = 256  # Adjust based on your dataset's token length\n",
    "LOG_FILE = os.path.join(OUTPUT_DIR, \"training_log.txt\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c0159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    LlavaForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc894df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:115] data. DefaultCPUAllocator: not enough memory: you tried to allocate 28253708288 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(OUTPUT_DIR, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Load model and processor on CPU\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlavaForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Full precision for CPU\u001b[39;49;00m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_ID)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Load and preprocess dataset\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vivek\\anaconda3\\envs\\capstone\\lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\Users\\vivek\\anaconda3\\envs\\capstone\\lib\\site-packages\\transformers\\modeling_utils.py:4400\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4390\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4391\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   4393\u001b[0m     (\n\u001b[0;32m   4394\u001b[0m         model,\n\u001b[0;32m   4395\u001b[0m         missing_keys,\n\u001b[0;32m   4396\u001b[0m         unexpected_keys,\n\u001b[0;32m   4397\u001b[0m         mismatched_keys,\n\u001b[0;32m   4398\u001b[0m         offload_index,\n\u001b[0;32m   4399\u001b[0m         error_msgs,\n\u001b[1;32m-> 4400\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4409\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4418\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[0;32m   4419\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32mc:\\Users\\vivek\\anaconda3\\envs\\capstone\\lib\\site-packages\\transformers\\modeling_utils.py:4793\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[0;32m   4791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4792\u001b[0m     expanded_device_map \u001b[38;5;241m=\u001b[39m expand_device_map(device_map, expected_keys)\n\u001b[1;32m-> 4793\u001b[0m     \u001b[43mcaching_allocator_warmup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_device_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4795\u001b[0m error_msgs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   4796\u001b[0m \u001b[38;5;66;03m# Iterate on all the shards to load the weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vivek\\anaconda3\\envs\\capstone\\lib\\site-packages\\transformers\\modeling_utils.py:5799\u001b[0m, in \u001b[0;36mcaching_allocator_warmup\u001b[1;34m(model, expanded_device_map, factor)\u001b[0m\n\u001b[0;32m   5797\u001b[0m     byte_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(byte_count, \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.95\u001b[39m \u001b[38;5;241m*\u001b[39m device_memory))\n\u001b[0;32m   5798\u001b[0m \u001b[38;5;66;03m# Allocate memory\u001b[39;00m\n\u001b[1;32m-> 5799\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:115] data. DefaultCPUAllocator: not enough memory: you tried to allocate 28253708288 bytes."
     ]
    }
   ],
   "source": [
    "# Load model and processor on CPU\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float32,  # Full precision for CPU\n",
    "    device_map=\"cpu\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    images = []\n",
    "    input_ids = []\n",
    "    labels = []\n",
    "\n",
    "    for example in zip(examples[\"image\"], examples[\"conversations\"]):\n",
    "        image_path = os.path.join(IMAGE_FOLDER, example[0])\n",
    "        conversation = example[1]\n",
    "\n",
    "        # Format conversation into a prompt\n",
    "        prompt = \"\"\n",
    "        for turn in conversation:\n",
    "            role = \"USER\" if turn[\"from\"] == \"human\" else \"ASSISTANT\"\n",
    "            prompt += f\"{role}: {turn['value']}\\n\"\n",
    "            if role == \"USER\" and \"<image>\" in turn[\"value\"]:\n",
    "                prompt = prompt.replace(\"<image>\", \"\")\n",
    "\n",
    "        # Process inputs\n",
    "        inputs = processor(\n",
    "            text=prompt,\n",
    "            images=[image_path],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        images.append(inputs[\"pixel_values\"].squeeze(0))\n",
    "        input_ids.append(inputs[\"input_ids\"].squeeze(0))\n",
    "        labels.append(inputs[\"input_ids\"].squeeze(0))  # Labels are same as input_ids for causal LM\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": images,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "# Apply preprocessing\n",
    "dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,  # Small batch size for CPU\n",
    "    gradient_accumulation_steps=8,  # Accumulate to simulate larger batch size\n",
    "    learning_rate=1e-5,  # Lower learning rate for stability\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=OUTPUT_DIR,\n",
    "    logging_strategy=\"steps\",\n",
    "    log_level=\"info\",\n",
    "    report_to=\"none\",  # Disable Weights & Biases\n",
    "    no_cuda=True,  # Explicitly disable GPU\n",
    "    fp16=False,  # No mixed precision on CPU\n",
    "    evaluation_strategy=\"no\",  # Add validation set if available\n",
    ")\n",
    "\n",
    "# Custom logging to file\n",
    "def log_to_file(trainer, step, metrics):\n",
    "    with open(LOG_FILE, \"a\") as f:\n",
    "        f.write(f\"Step {step}: {metrics}\\n\")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=lambda x: {\n",
    "        \"pixel_values\": torch.stack([item[\"pixel_values\"] for item in x]),\n",
    "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in x]),\n",
    "        \"labels\": torch.stack([item[\"labels\"] for item in x]),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Override logging to write to file\n",
    "trainer.log = lambda logs: log_to_file(trainer, trainer.state.global_step, logs)\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fccaa2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
